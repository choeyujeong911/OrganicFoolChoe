{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.467176Z",
     "start_time": "2025-09-18T06:04:57.462304Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" Fashion-MNIST 예측 + 시각화 스크립트\n",
    "- 학습된 모델(best_fashion_cnn.pt) 로드\n",
    "- 임의 이미지(내가 찍은 사진) 전처리 → 예측 → 요약 이미지 저장\n",
    "- 결과물: ./outputs_fashion/<이름>_preprocessed.png, <이름>_summary.png,\n",
    "summary_all.png\n",
    "\"\"\"\n",
    "import os, math, argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.476678Z",
     "start_time": "2025-09-18T06:04:57.472312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== 기본 설정(고정 실행용 기본값) =====\n",
    "DEFAULT_MODEL = \"./outputs_fashion/best_fashion_cnn.pt\"\n",
    "DEFAULT_IMAGES = [\"신발.jpg\", \"셔츠.jpg\"] # 여기에 본인 파일명 넣어도 됨\n",
    "DEFAULT_OUTDIR = \"./outputs_fashion\"\n",
    "CLASS_NAMES = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]"
   ],
   "id": "4539f43853e4dfa4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.486106Z",
     "start_time": "2025-09-18T06:04:57.476678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== 모델 정의 (훈련 코드와 동일) =====\n",
    "class Block(nn.Module):\n",
    " def __init__(self, in_c, out_c):\n",
    "  super().__init__()\n",
    "  self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "  self.bn1 = nn.BatchNorm2d(out_c)\n",
    "  self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)\n",
    "  self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    " def forward(self, x):\n",
    "  x = F.relu(self.bn1(self.conv1(x)))\n",
    "  x = F.relu(self.bn2(self.conv2(x)))\n",
    "  return x\n",
    "\n",
    "\n",
    "class FashionCNN(nn.Module):\n",
    " def __init__(self, dropout=0.3):\n",
    "  super().__init__()\n",
    "  self.block1 = Block(1, 32) # 28x28\n",
    "  self.pool1 = nn.MaxPool2d(2,2) # 14x14\n",
    "  self.block2 = Block(32, 64) # 14x14\n",
    "  self.pool2 = nn.MaxPool2d(2,2) # 7x7\n",
    "  self.fc1 = nn.Linear(64*7*7, 256)\n",
    "  self.drop = nn.Dropout(dropout)\n",
    "  self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    " def forward(self, x):\n",
    "  x = self.pool1(self.block1(x))\n",
    "  x = self.pool2(self.block2(x))\n",
    "  x = x.view(x.size(0), -1)\n",
    "  x = F.relu(self.fc1(x))\n",
    "  x = self.drop(x)\n",
    "  return self.fc2(x)"
   ],
   "id": "e6c254795bb4efae",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.494452Z",
     "start_time": "2025-09-18T06:04:57.490007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _otsu_threshold(gray_u8: np.ndarray) -> int:\n",
    " hist, _ = np.histogram(gray_u8, bins=256, range=(0,256))\n",
    " total = gray_u8.size\n",
    " sum_total = np.dot(np.arange(256), hist)\n",
    " sum_b = w_b = max_var = 0.0\n",
    " thr = 0\n",
    " for t in range(256):\n",
    "  w_b += hist[t]\n",
    "  if w_b == 0: continue\n",
    "  w_f = total - w_b\n",
    "  if w_f == 0: break\n",
    "  sum_b += t * hist[t]\n",
    "  m_b = sum_b / w_b\n",
    "  m_f = (sum_total - sum_b) / w_f\n",
    "  var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "  if var_between > max_var:\n",
    "   max_var = var_between\n",
    "   thr = t\n",
    "  return thr\n"
   ],
   "id": "5413ddddd0e1ebf2",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.503707Z",
     "start_time": "2025-09-18T06:04:57.498068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_photo_to_fashion(path: str, save_preview: str | None = None):\n",
    " \"\"\"\n",
    " 1) Grayscale → 2) Otsu 이진화로 전경(BBOX) → 3) 정사각 패딩(여백 20%)\n",
    " 4) 28×28 리사이즈 → 5) Tensor/Normalize(mean=0.2861, std=0.3530)\n",
    " 반환:\n",
    " tens_norm: (1,28,28) float 텐서\n",
    " pre_u8: 28×28 uint8 (시각화용)\n",
    " \"\"\"\n",
    " im = Image.open(path).convert(\"L\")\n",
    " arr = np.array(im, dtype=np.uint8)\n",
    "\n",
    " # 전경(옷) 추정\n",
    " th = _otsu_threshold(arr)\n",
    " # 일반적으로 배경이 더 밝고, 물체(옷)가 더 어두운 경우가 많음\n",
    " bin_fg = (arr < th).astype(np.uint8)\n",
    " if bin_fg.sum() < 100: # 실패 시 반전 가정\n",
    "  bin_fg = (arr > th).astype(np.uint8)\n",
    "\n",
    " ys, xs = np.where(bin_fg > 0)\n",
    " if len(xs) == 0 or len(ys) == 0:\n",
    "  crop = arr # 전경 못 찾으면 전체 사용\n",
    " else:\n",
    "  x0, x1 = xs.min(), xs.max()\n",
    "  y0, y1 = ys.min(), ys.max()\n",
    "  crop = arr[y0:y1+1, x0:x1+1]\n",
    "\n",
    " # 정사각 패딩 + 여백 20%\n",
    " h, w = crop.shape\n",
    " size = int(max(h, w) * 1.2)\n",
    " canvas = np.full((size, size), 255, dtype=np.uint8) # 흰 배경\n",
    " y_off = (size - h) // 2\n",
    " x_off = (size - w) // 2\n",
    " canvas[y_off:y_off+h, x_off:x_off+w] = crop\n",
    "\n",
    " # 28×28 리사이즈\n",
    " arr28 = np.array(Image.fromarray(canvas).resize((28, 28), Image.BILINEAR), dtype=np.uint8)\n",
    "\n",
    " # Tensor + Normalize(Fashion-MNIST 권장)\n",
    " tens = torch.from_numpy(arr28).float().unsqueeze(0) / 255.0 # [0,1]\n",
    " mean, std = 0.2861, 0.3530\n",
    " tens_norm = (tens - mean) / std # (1,28,28)\n",
    " if save_preview:\n",
    "  Image.fromarray(arr28).save(save_preview)\n",
    " return tens_norm, arr28"
   ],
   "id": "2b40d399a7e39089",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.512320Z",
     "start_time": "2025-09-18T06:04:57.503707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== 예측/시각화 =====\n",
    "@torch.no_grad()\n",
    "def predict(model, tens_norm, device):\n",
    " x = tens_norm.unsqueeze(0).to(device) # (1,1,28,28)\n",
    " logits = model(x)\n",
    " probs = torch.softmax(logits, dim=1).cpu().numpy().ravel()\n",
    " pred = int(probs.argmax())\n",
    " return pred, probs\n",
    "\n",
    "def topk(probs: np.ndarray, k=3):\n",
    " idx = probs.argsort()[-k:][::-1]\n",
    " return [(int(i), float(probs[i])) for i in idx]\n",
    "\n",
    "def topk_str(probs: np.ndarray, k=3):\n",
    " items = topk(probs, k)\n",
    " return \", \".join([f\"{CLASS_NAMES[i]}:{p:.3f}\" for i,p in items])"
   ],
   "id": "6d225344e8c12709",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.521236Z",
     "start_time": "2025-09-18T06:04:57.514068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_summary(orig_path: str, pre_u8: np.ndarray, pred: int, probs: np.ndarray, save_path: str):\n",
    " plt.figure(figsize=(9,4))\n",
    " # 원본\n",
    " plt.subplot(1,2,1)\n",
    " img = Image.open(orig_path).convert(\"RGB\")\n",
    " plt.imshow(img); plt.title(\"Original\"); plt.axis(\"off\")\n",
    " # 전처리 + 결과\n",
    " plt.subplot(1,2,2)\n",
    " plt.imshow(pre_u8, cmap=\"gray\", interpolation=\"nearest\")\n",
    " plt.title(f\"Preprocessed (28×28)\\nPred: {CLASS_NAMES[pred]}\\nTop-3:{topk_str(probs,3)}\")\n",
    " plt.axis(\"off\")\n",
    " plt.tight_layout(); plt.savefig(save_path, dpi=150); plt.close()\n",
    "\n",
    "def make_all_grid(summary_paths, save_path, cols=2):\n",
    " if not summary_paths: return\n",
    " ims = [Image.open(p).convert(\"RGB\") for p in summary_paths]\n",
    " w, h = ims[0].size\n",
    " rows = int(np.ceil(len(ims)/cols))\n",
    " canvas = Image.new(\"RGB\", (cols*w, rows*h), (255,255,255))\n",
    " for i, im in enumerate(ims):\n",
    "  r, c = divmod(i, cols)\n",
    "  canvas.paste(im, (c*w, r*h))\n",
    " canvas.save(save_path)"
   ],
   "id": "5ab133eb466b53c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.530899Z",
     "start_time": "2025-09-18T06:04:57.521236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== 메인 =====\n",
    "def run(model_path: str, images: list[str], out_dir: str):\n",
    " device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " os.makedirs(out_dir, exist_ok=True)\n",
    " assert os.path.exists(model_path), f\"모델이 없습니다: {model_path}\"\n",
    "\n",
    " model = FashionCNN()\n",
    " model.load_state_dict(torch.load(model_path, map_location=device))\n",
    " model.to(device).eval()\n",
    " print(f\"✅ 모델 로드 완료: {model_path}\")\n",
    "\n",
    " summary_paths = []\n",
    " for path in images:\n",
    "  assert os.path.exists(path), f\"이미지 없음: {path}\"\n",
    "  base = os.path.splitext(os.path.basename(path))[0]\n",
    "  pre_path = os.path.join(out_dir, f\"{base}_preprocessed.png\")\n",
    "  sum_path = os.path.join(out_dir, f\"{base}_summary.png\")\n",
    "\n",
    "  tens_norm, pre_u8 = preprocess_photo_to_fashion(path, save_preview=pre_path)\n",
    "  pred, probs = predict(model, tens_norm, device)\n",
    "\n",
    "  print(f\"\\n {path}\")\n",
    "  print(f\" - Pred : {CLASS_NAMES[pred]} ({pred})\")\n",
    "  print(f\" - Top-3: {topk_str(probs, 3)}\")\n",
    "  print(f\" - Preprocessed saved: {pre_path}\")\n",
    "\n",
    "  make_summary(path, pre_u8, pred, probs, sum_path)\n",
    "  print(f\" - Summary saved: {sum_path}\")\n",
    "  summary_paths.append(sum_path)\n",
    "\n",
    " grid_path = os.path.join(out_dir, \"summary_all.png\")\n",
    " make_all_grid(summary_paths, grid_path, cols=2)\n",
    " print(f\"\\n 전체 요약 그리드: {grid_path}\")"
   ],
   "id": "c926ecd7d73ec770",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T06:04:57.795457Z",
     "start_time": "2025-09-18T06:04:57.530899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_args():\n",
    " ap = argparse.ArgumentParser()\n",
    " ap.add_argument(\"--model\", type=str, default=DEFAULT_MODEL)\n",
    " ap.add_argument(\"--images\", nargs=\"*\", default=DEFAULT_IMAGES)\n",
    " ap.add_argument(\"--out-dir\", type=str, default=DEFAULT_OUTDIR)\n",
    " return ap.parse_known_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    " args, _ = parse_args()\n",
    " run(args.model, args.images, args.out_dir)"
   ],
   "id": "8303026f7d94fa12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 완료: ./outputs_fashion/best_fashion_cnn.pt\n",
      "\n",
      " 신발.jpg\n",
      " - Pred : Bag (8)\n",
      " - Top-3: Bag:0.995, T-shirt/top:0.002, Trouser:0.001\n",
      " - Preprocessed saved: ./outputs_fashion\\신발_preprocessed.png\n",
      " - Summary saved: ./outputs_fashion\\신발_summary.png\n",
      "\n",
      " 셔츠.jpg\n",
      " - Pred : Bag (8)\n",
      " - Top-3: Bag:0.871, T-shirt/top:0.055, Ankle boot:0.020\n",
      " - Preprocessed saved: ./outputs_fashion\\셔츠_preprocessed.png\n",
      " - Summary saved: ./outputs_fashion\\셔츠_summary.png\n",
      "\n",
      " 전체 요약 그리드: ./outputs_fashion\\summary_all.png\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
